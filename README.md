# Projeto de Computa√ß√£o em Nuvem

Projeto de implementa√ß√£o de um ALB entre inst√¢ncias EC2 com Auto Scaling e Banco de dados RDS

**Por: Renato Laffranchi Falc√£o**

## Objetivo de Implementa√ß√£o

![diagrama de arquitetura](img/arquitetura.png)

### Setup do ambiente de desenvolvimento

O projeto ser√° desenvolvido em Terraform, uma ferramenta de desenvolvimento de infraestrutura como c√≥digo (IaC), que ir√° gerenciar inst√¢ncias e servi√ßos da provedora AWS. O diret√≥rio do projeto pode ser estruturado da seguinte forma:

    .
    ‚îú‚îÄ‚îÄ terraform/
    ‚îÇ   ‚îú‚îÄ‚îÄ main.tf
    ‚îÇ   ‚îú‚îÄ‚îÄ ec2.tf
    ‚îÇ   ‚îú‚îÄ‚îÄ rds.tf
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ outputs.tf

Nesta estrutura de diret√≥rios, o arquivo **main.tf** cont√©m ...

O arquivo **outputs.tf** cont√©m ...

Antes de mais nada, √© necess√°rio realizar a instala√ß√£o do Terraform. Para tanto, basta seguir o [tutorial de instala√ß√£o do Terraform](https://developer.hashicorp.com/terraform/downloads) de acordo com o sistema operacional.

Al√©m do Terraform, tamb√©m √© preciso instalar a [CLI da AWS](https://aws.amazon.com/pt/cli/) de acordo com o sistema operacional. 

Por fim, deve-se configurar as credencias da AWS para que o Terraform realize todo o gerenciamento. A seguinte forma de fazer esta configura√ß√£o √© uma boa pr√°tica para evitar vazamento de credenciais caso os scripts sejam compartilhados. Utilizando o comando:

    aws configure

- `AWS Access Key ID`¬†e¬†`AWS Secret Access Key ID`: O¬†**ID**¬†e a¬†**chave de acesso**¬†gerados no console da AWS, na aba¬†*"Credenciais de Seguran√ßa"*.
- `Default region name`: A regi√£o padr√£o para se implantar servi√ßos e inst√¢ncias. Neste caso, est√° sendo utilizada a regi√£o **MUDAR A REGI√ÉO AQUI**.
- `Default output format`: O formato de sa√≠da padr√£o das respostas recebidas. Por simplicidade, **json**.

# MUDAR TUDO A PARTIR DAQUI

### Inicializa√ß√£o do projeto

Inicialmente, crie um diret√≥rio para guardar todos os arquivos do projeto:

    mkdir Projeto-Computacao-Nuvem
    mkdir Projeto-Computacao-Nuvem/terraform
    cd Projeto-Computacao-Nuvem/terraform

Agora crie e acesse um arquivo chamado `main.tf` :

    touch main.tf
    sudo nano main.tf

<aside>
üí° Dentro deste arquivo colocaremos toda a nossa infraestrutura

</aside>

Primeiro, vamos iniciar o arquivo com o seguinte c√≥digo:

    terraform {
      required_providers {
        aws = {
          source  = "hashicorp/aws"
          version = "~> 4.0"
        }
      }
    }

    provider "aws" {
      alias   = "region_main"
      region  = "us-east-1"
      profile = "default"
    }

Com este bloco, estamos configurando o Terraform com o provedor que estaremos utilizando, que neste caso √© a AWS, especificando tamb√©m a vers√£o do provedor, a regi√£o na qual as aplica√ß√µes ser√£o implantadas, bem como um apelido para este provedor (‚Äùregion_main‚Äù). Al√©m do mais, este provedor √© definido como o provedor padr√£o. Isso significa que todos os servi√ßos e inst√¢ncias ser√£o implantados a partir deste provedor, nesta regi√£o, a menos que um outro provedor seja especificado.

### Configura√ß√£o de rede

Em seguida, vamos definir uma VPC (Virtual Private Cloud). A VPC √© uma rede virtual que isola o que est√° dentro dela para com o resto da internet. E √© dentro dela que toda a infraestrutura vai funcionar. Para cri√°-la, basta adicionar o seguinte bloco de c√≥digo:

    resource "aws_vpc" "app_vpc" {
      cidr_block = "172.16.0.0/16"
      tags = {
        Name = "app_vpc"
      }
    }

    resource "aws_internet_gateway" "app_gateway" {
      vpc_id = aws_vpc.app_vpc.id

      tags = {
        Name = "app_gateway"
      }
    }

Estes c√≥digos criam, respectivamente, a VPC, com bloco de endere√ßos na faixa de 172.16.0.0/16, e um Gateway de Internet, para que o mundo externo consiga se comunicar com a rede interna √† VPC.

A seguir, iremos configurar nossas duas sub-redes, nas quais as inst√¢ncias ser√£o implantadas. Para configurar sub-redes:

    resource "aws_subnet" "app_subnet_e1a" {
      vpc_id                  = aws_vpc.app_vpc.id
      cidr_block              = "172.16.64.0/19"
      availability_zone       = "us-east-1a"
      map_public_ip_on_launch = true

      tags = {
        Name = "app_public_subnet_e1a"
      }
    }

    resource "aws_subnet" "app_subnet_e1b" {
      vpc_id                  = aws_vpc.app_vpc.id
      cidr_block              = "172.16.96.0/19"
      availability_zone       = "us-east-1b"
      map_public_ip_on_launch = true

      tags = {
        Name = "app_public_subnet_e1b"
      }
    }

Estas sub-redes est√£o configuradas dentro da VPC, portanto na mesma regi√£o, mas em zonas de disponibilidade diferentes. Est√£o configuradas para utilizarem os blocos de endere√ßo 172.16.64.0/19 e 172.16.96.0/19, de forma ‚Äúp√∫blica‚Äù, ou seja, mapeando IPs p√∫blicos quando inst√¢ncias s√£o criadas nestas sub-redes. Esta configura√ß√£o √© um primeiro passo para permitir que acessemos tais inst√¢ncias via SSH, caso desejemos.

Ap√≥s a cria√ß√£o das sub-redes, precisamos configurar uma tabela de roteamento, que vai rotear e direcionar o tr√°fego dentro da VPC para as devidas sub-redes. Para isso vamos adicionar o seguinte c√≥digo:

    resource "aws_route_table" "app_route_table" {
      vpc_id = aws_vpc.app_vpc.id

      route {
        gateway_id = aws_internet_gateway.app_gateway.id
        cidr_block = "0.0.0.0/0"
      }

      tags = {
        Name = "app_route_table"
      }

    }

    resource "aws_route_table_association" "app_route_table_association_e1a" {
      subnet_id      = aws_subnet.app_subnet_e1a.id
      route_table_id = aws_route_table.app_route_table.id
    }

    resource "aws_route_table_association" "app_route_table_association_e1b" {
      subnet_id      = aws_subnet.app_subnet_e1b.id
      route_table_id = aws_route_table.app_route_table.id
    }

Desta forma, tanto as sub-redes criadas, como o gateway de internet s√£o registrados na tabela de roteamento, permitindo que o tr√°fego seja direcionado corretamente.  

### Configura√ß√£o dos grupos de seguran√ßa

A partir do momento em que temos a rede configurada, podemos come√ßar a pensar na implanta√ß√£o das inst√¢ncias e do load balancer. No entanto, para que elas funcionem corretamente, √© necess√°rio que sejam configurados os grupos de seguran√ßa, bem como suas regras. Esses grupos de seguran√ßa definem as permiss√µes que as inst√¢ncias ou servi√ßos tem para receber ou enviar pacotes. Em geral, essas regras definem, principalmente, portas de acesso permitidas, protocolos de comunica√ß√£o e endere√ßos que podem fazer as requisi√ß√µes.

Primeiramente, vamos criar dois grupos de seguran√ßa: um para as inst√¢ncias e um para o load balancer:

    resource "aws_security_group" "app_security_group" {
      name   = "app_security_group"
      vpc_id = aws_vpc.app_vpc.id
    }

    resource "aws_security_group" "lb_security_group" {
      name   = "lb_security_group"
      vpc_id = aws_vpc.app_vpc.id
    }

Agora, vamos criar as regras que definir√£o o comportamento das inst√¢ncias:

    resource "aws_security_group_rule" "app_ingress_rule" {
      type              = "ingress"
      from_port         = 80
      to_port           = 80
      protocol          = "tcp"
      security_group_id = aws_security_group.app_security_group.id
      cidr_blocks       = ["0.0.0.0/0"]
    }

    resource "aws_security_group_rule" "app_ingress_rule_ssh" {
      type              = "ingress"
      from_port         = 22
      to_port           = 22
      protocol          = "tcp"
      security_group_id = aws_security_group.app_security_group.id
      cidr_blocks       = ["0.0.0.0/0"]
    }

A primeira regra define que as inst√¢ncias podem **************receber************** dados na porta 80, por protocolo TCP (isso inclui a permiss√£o de protocolos de comunica√ß√£o de mais alto n√≠vel, como HTTP e HTTPS). A permiss√£o √© dada para que as inst√¢ncias recebam dados de qualquer IP.

J√° a segunda regra define que as inst√¢ncias podem **************receber************** dados na porta 22, que √© a padr√£o para acesso de SSH, por protocolo TCP, tamb√©m de qualquer IP.

E pensando nas regras para definir o comportamento do load balancer, temos:

    resource "aws_security_group_rule" "lb_ingress_rule" {
      type              = "ingress"
      from_port         = "-1"
      to_port           = "-1"
      protocol          = "-1"
      security_group_id = aws_security_group.lb_security_group.id
      cidr_blocks       = ["0.0.0.0/0"]
    }

    resource "aws_security_group_rule" "lb_egress_rule" {
      type              = "egress"
      from_port         = "-1"
      to_port           = "-1"
      protocol          = "-1"
      security_group_id = aws_security_group.lb_security_group.id
      cidr_blocks       = ["0.0.0.0/0"]
    }

Estas regras, resumo, permitem que o load balancer tanto ************receba************ quanto **********envie********** dados, atrav√©s de ********todas******** as suas portas, de ****************************************qualquer endere√ßo IP****************************************. Isso abre um maior leque de possibilidades para o futuro, caso se deseje adicionar novas aplica√ß√µes para o load balancer ou at√© mesmo acess√°-lo de diferentes lugares do mundo, dando maior flexibilidade para este servi√ßo. 

### Configura√ß√£o das inst√¢ncias

Finalizadas as configura√ß√µes dos grupos de seguran√ßa, vamos come√ßar pela implanta√ß√£o das inst√¢ncias, que carregar√£o uma aplica√ß√£o Wordpress. Vamos utilizar um bloco que permitir√° a cria√ß√£o das inst√¢ncias de forma simplificada e pode ser usada para replicar o processo de cria√ß√£o de mais inst√¢ncias:

    locals {
      apps = {
        app_1 = {
          machine_type = "t3a.small"
          subnet_id    = aws_subnet.app_subnet_e1a.id
        }
        app_2 = {
          machine_type = "t3a.small"
          subnet_id    = aws_subnet.app_subnet_e1b.id
        }
      }
    }

Este bloco define as caracter√≠sticas exclusivas das inst√¢ncias que ser√£o criadas, permitindo que utilizemos um loop ‚Äúfor_each‚Äù do Terraform para criar as inst√¢ncias:

    resource "aws_instance" "app_instance" {
      for_each = local.apps

      ami           = "ami-0d5627e38262b3304"
      instance_type = each.value.machine_type
      subnet_id     = each.value.subnet_id

      vpc_security_group_ids = [aws_security_group.app_security_group.id]

      tags = {
        Name = each.key
      }
    }

Estas configura√ß√µes permitem que duas inst√¢ncias sejam criadas nas duas diferentes zonas de disponibilidade, com seus respectivos tipos de m√°quina, ambas utilizando do mesmo grupo de seguran√ßa criado na etapa anterior. AMI‚Äôs (Amazon Machine Images ou Imagens de M√°quina da Amazon) s√£o imagens mantidas pela AWS que fornecem as informa√ß√µes de sistema operacional e execu√ß√£o de aplica√ß√µes necess√°rias para iniciar uma inst√¢ncia. Desta forma, a imagem escolhida foi uma que j√° realiza a implanta√ß√£o autom√°tica do Wordpress em sistema operacional Debian.

### Configura√ß√£o do Load Balancer

Finalmente podemos come√ßar a configura√ß√£o do Load Balancer. O servi√ßo de load balancing √© dividido em algumas partes. Primeiramente, √© necess√°rio configurar um ‚ÄúTarget Group‚Äù (ou grupo de destino) do balancer. O grupo de destino √© usado para rotear o tr√°fego entre o balanceador de carga e os servidores de destino registrados nele:

    resource "aws_lb_target_group" "app_target_group" {
      name     = "apptargetgroup"
      port     = 80
      protocol = "HTTP"
      vpc_id   = aws_vpc.app_vpc.id

      load_balancing_algorithm_type = "round_robin"

      health_check {
        enabled             = true
        port                = 80
        interval            = 30
        protocol            = "HTTP"
        path                = "/"
        matcher             = "200"
        healthy_threshold   = 3
        unhealthy_threshold = 3
      }
    }

O target group define a porta na qual o tr√°fego ser√° roteado para os servidores de destino no grupo de destino, que neste caso ser√° encaminhado para a porta 80, define o protocolo que ser√° usado para o tr√°fego entre o balanceador de carga e os servidores de destino, neste caso o HTTP, especifica o ID da VPC na qual o grupo de destino ser√° criado, define o algoritmo de balanceamento de carga que ser√° usado pelo grupo de destino e define, na se√ß√£o health_check, as configura√ß√µes de verifica√ß√£o de integridade para os servidores de destino no grupo de destino, que √© usado para determinar se um servidor de destino est√° saud√°vel e pode receber tr√°fego.

Em seguida, deve-se ‚Äúacoplar‚Äù as inst√¢ncias criadas ao grupo de destino:

    resource "aws_lb_target_group_attachment" "app_target_group_attachment" {
      for_each = aws_instance.app_instance

      target_group_arn = aws_lb_target_group.app_target_group.arn
      target_id        = each.value.id
      port             = 80
    }

Este c√≥digo usa novamente o loop for_each para ‚Äúacoplar‚Äù ambas as inst√¢ncias grupo de destino, especificando seu ARN (Amazon Resource Name ou Nome de Recurso da Amazon) e o id da inst√¢ncia.

Agora √© hora de criar o recurso do load balancer propriamente dito, que neste caso ser√° configurado como um load balancer do tipo ‚Äúaplica√ß√£o‚Äù, isto √©, opera na camada de aplica√ß√£o:

    resource "aws_lb" "app_elb" {
      name               = "app-elb"
      internal           = false
      load_balancer_type = "application"
      security_groups    = [aws_security_group.lb_security_group.id]

      subnets = [
        aws_subnet.app_subnet_e1a.id,
        aws_subnet.app_subnet_e1b.id
      ]
    }

Este recurso define se o balanceador de carga √© interno ou externo, ou seja, se ele pode ser visto apenas pela rede interna da VPC ou tamb√©m pela rede externa. Neste caso, queremos que ela esteja vis√≠vel pela rede externa tamb√©m, para podermos acess√°-la. Al√©m disso, o grupo de seguran√ßa criado anteriormente √© associado ao recurso e as sub-redes de opera√ß√£o s√£o especificadas.

Ent√£o, para tornar o load balancer dispon√≠vel para receber requisi√ß√µes e encaminh√°-las √†s inst√¢ncias, podemos utilizar o recurso de ‚Äúlistener‚Äù a seguir:

    resource "aws_lb_listener" "app_listener" {
      load_balancer_arn = aws_lb.app_elb.arn
      port              = "80"
      protocol          = "HTTP"

      default_action {
        type             = "forward"
        target_group_arn = aws_lb_target_group.app_target_group.arn
      }
    }

Neste c√≥digo, o recurso de load balancing a que ele est√° associado √© declarado, definindo tamb√©m a porta em que est√° escutando as requisi√ß√µes, neste caso √© a porta 80,  o protocolo utilizado, o HTTP, e o grupo para o qual o tr√°fego ser√° direcionado. Desta forma, o listener √© respons√°vel apenas por encaminhar os dados, sendo responsabilidade do algoritmo do load balancer de dividir o tr√°fego.

Agora salve e feche o arquivo `main.tf`.

### Finaliza√ß√£o e Implanta√ß√£o

Este c√≥digo em Terraform √© o suficiente para implantar toda a infraestrutura necess√°ria. No entanto, para se acessar a aplica√ß√£o, atrav√©s do load balancer, precisamos recuperar de alguma forma o nome do DNS p√∫blico no qual a aplica√ß√£o est√° dispon√≠vel. Para isso, vamos utilizar os ‚Äúoutputs‚Äù do Terraform.

Crie e acesse um arquivo chamado `outputs.tf`:

    touch outputs.tf
    sudo nano outputs.tf

Neste arquivo, vamos colocar o seguinte bloco de c√≥digo:

    output "lb_url" {
      description = "URL do Load Balancer"
      value       = "http://${aws_lb.app_elb.dns_name}/"
    }

Este c√≥digo vai fazer com que, ao final da implanta√ß√£o, a URL da nossa aplica√ß√£o seja imprimida no terminal. Desta forma, basta clicar e voc√™ ser√° redirecionado para a sua aplica√ß√£o funcionando na nuvem, com o uso de um Load Balancer.

Agora salve e feche o arquivo `outputs.tf`.

Para implementar toda a infraestrutura, inicie o Terraform com o provedor requerido. Dentro do diret√≥rio `terraform/`:

    terraform init

Com o provedor inicializado, podemos criar um "plano", isto √©, uma forma de analisar todas as mudan√ßas que ser√£o implementadas na infraestrutura antes de executar na AWS. Neste "plano" ele demonstra tudo que ser√° adicionado, alterado ou destru√≠do na sua infraestrutura com um toque para facilitar a visualiza√ß√£o:

I. to add --- tr√°s um s√≠mbolo de adi√ß√£o (+) em verde para demostrar o que ser√° criado.

II. to change --- tr√°s um s√≠mbolo de til (~) em amarelo para demostrar mudan√ßa de estado do recurso criado.

III. to destroy --- tr√°s um s√≠mbolo de subtra√ß√£o (-) em vermelho para demonstrar o que ser√° exclu√≠do da infraestrutura.

Desta forma, √© poss√≠vel "debugar" erros de programa√ß√£o e analisar respostas de recursos a serem criados, antes de subir a estrutura para a AWS, bastando alterar os arquivos e recursos que desejar, salvar os documentos alterados e rodar o seguinte comando at√© ficar satisfeito com o "plano" de cria√ß√£o:

    terraform plan -out plano

Depois de analisar se √© isso mesmo que deseja criar, √© s√≥ utilizar o comando:

    terraform apply "plano"

O processo de implanta√ß√£o pode demorar alguns poucos minutos, mas quando finalizar, voc√™ deve ver a URL para sua aplica√ß√£o impressa no terminal no formato:

    lb_url: http://DNS_SEU_APP/

Quando quiser finalizar toda a infraestrutura, apenas execute o comando:

    terraform destroy